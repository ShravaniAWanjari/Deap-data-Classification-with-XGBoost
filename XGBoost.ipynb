{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, mean_absolute_error\n",
    "from scipy.signal import welch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 256 #Averaging band power of 2 sec\n",
    "step_size = 16 #Each 0.125 sec update once\n",
    "sample_rate = 128 #Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_power(data, band, sample_rate):\n",
    "    \"\"\"Calculate band power for a given signal.\"\"\"\n",
    "    freqs, psd = welch(data, sample_rate, nperseg=sample_rate*2)\n",
    "    bandpower = []\n",
    "    for (low, high) in zip(band[:-1], band[1:]):\n",
    "        idx_band = np.logical_and(freqs >= low, freqs < high)\n",
    "        bandpower.append(np.sum(psd[idx_band]))\n",
    "    return bandpower\n",
    "\n",
    "def FFT_Processing(sub, channel, band, window_size, step_size, sample_rate):\n",
    "    '''\n",
    "    arguments:  string subject\n",
    "                list channel indices\n",
    "                list band\n",
    "                int window size for FFT\n",
    "                int step size for FFT\n",
    "                int sample rate for FFT\n",
    "    return:     void\n",
    "    '''\n",
    "    meta = []\n",
    "    with open(f'data_preprocessed_python/s{sub}.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1')  \n",
    "\n",
    "        for i in range(40): \n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            start = 0\n",
    "\n",
    "            while start + window_size < data.shape[1]:\n",
    "                meta_array = []\n",
    "                meta_data = []  \n",
    "                for j in channel:\n",
    "                    X = data[j][start: start + window_size] \n",
    "                    Y = band_power(X, band, sample_rate)  \n",
    "                    meta_data = meta_data + list(Y)\n",
    "\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                meta_array.append(labels)\n",
    "\n",
    "                meta.append(np.array(meta_array))    \n",
    "                start += step_size\n",
    "                \n",
    "        meta = np.array(meta)\n",
    "\n",
    "        if not os.path.exists('out'):\n",
    "            os.makedirs('out')\n",
    "\n",
    "        np.save(f'out/s{sub}', meta, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "\n",
    "\n",
    "def testing(M, L, model):\n",
    "    '''\n",
    "    arguments:  M: testing dataset\n",
    "                L: testing dataset label\n",
    "                model: scikit-learn model\n",
    "\n",
    "    return:     void\n",
    "    '''\n",
    "    output = model.predict(M[0:468480:32])\n",
    "    label = L[0:468480:32]\n",
    "\n",
    "    k = 0\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        k = k + (output[i] - label[i]) ** 2 \n",
    "\n",
    "      \n",
    "        if (output[i] > 5 and label[i] > 5) or (output[i] < 5 and label[i] < 5):\n",
    "            l += 1\n",
    "\n",
    "    print(\"l2 error:\", k / len(label), \"classification accuracy:\", l / len(label), l, len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subjects in subjectList:\n",
    "    FFT_Processing(subjects, channel, band, window_size, step_size, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset: (468480, 70) (468480, 4)\n",
      "testing dataset: (78080, 70) (78080, 4)\n",
      "validation dataset: (78080, 70) (78080, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "data_validation = []\n",
    "label_validation = []\n",
    "\n",
    "for subjects in subjectList:\n",
    "    file_path = os.path.join('out', f's{subjects}.npy')\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        sub = np.load(file, allow_pickle=True)\n",
    "        for i in range(sub.shape[0]):\n",
    "            if i % 8 == 0:\n",
    "                data_testing.append(sub[i][0])\n",
    "                label_testing.append(sub[i][1])\n",
    "            elif i % 8 == 1:\n",
    "                data_validation.append(sub[i][0])\n",
    "                label_validation.append(sub[i][1])\n",
    "            else:\n",
    "                data_training.append(sub[i][0])\n",
    "                label_training.append(sub[i][1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.save(os.path.join('out', 'data_training'), np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save(os.path.join('out', 'label_training'), np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "np.save(os.path.join('out', 'data_testing'), np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save(os.path.join('out', 'label_testing'), np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
    "\n",
    "np.save(os.path.join('out', 'data_validation'), np.array(data_validation), allow_pickle=True, fix_imports=True)\n",
    "np.save(os.path.join('out', 'label_validation'), np.array(label_validation), allow_pickle=True, fix_imports=True)\n",
    "print(\"validation dataset:\", np.array(data_validation).shape, np.array(label_validation).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23860\\954364482.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with open('out\\data_training.npy', 'rb') as fileTrain:\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23860\\954364482.py:4: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  with open('out\\label_training.npy', 'rb') as fileTrainL:\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23860\\954364482.py:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with open('out\\data_validation.npy', 'rb') as fileTrain:\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23860\\954364482.py:20: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  with open('out\\label_validation.npy', 'rb') as fileTrainL:\n"
     ]
    }
   ],
   "source": [
    "with open('out\\data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain)\n",
    "    \n",
    "with open('out\\label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL)\n",
    "    \n",
    "X = normalize(X)\n",
    "Z = np.ravel(Y[:, [1]])\n",
    "\n",
    "Arousal_Train = np.ravel(Y[:, [0]])\n",
    "Valence_Train = np.ravel(Y[:, [1]])\n",
    "Dominance_Train = np.ravel(Y[:, [2]])\n",
    "Liking_Train = np.ravel(Y[:, [3]])\n",
    "\n",
    "\n",
    "\n",
    "with open('out\\data_validation.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain)\n",
    "    \n",
    "with open('out\\label_validation.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL)\n",
    "\n",
    "M = normalize(M)\n",
    "L = np.ravel(N[:, [1]])\n",
    "\n",
    "Arousal_Test = np.ravel(N[:, [0]])\n",
    "Valence_Test = np.ravel(N[:, [1]])\n",
    "Dominance_Test = np.ravel(N[:, [2]])\n",
    "Liking_Test = np.ravel(N[:, [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.6882662254605978\n",
      "Classification Accuracy: 0.8389344262295082\n",
      "Standard Deviation of Predictions: 1.2896639108657837\n",
      "F1 Score: 0.858276235124414\n",
      "Mean Absolute Error: 0.9997934500272158\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=17, learning_rate=0.1, n_jobs=6)\n",
    "\n",
    "xgb_reg.fit(X[0:468480:32], Arousal_Train[0:468480:32])\n",
    "\n",
    "\n",
    "predictions = xgb_reg.predict(M[0:78080:32])\n",
    "\n",
    "\n",
    "mse = mean_squared_error(Arousal_Test[0:78080:32], predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "accuracy = accuracy_score((Arousal_Test[0:78080:32] > 5), (predictions > 5))\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "std_dev = np.std(predictions)\n",
    "print(f\"Standard Deviation of Predictions: {std_dev}\")\n",
    "\n",
    "f1 = f1_score((Arousal_Test[0:78080:32] > 5), (predictions > 5))\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "mae = mean_absolute_error(Arousal_Test[0:78080:32], predictions)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.5336520668496618\n",
      "Classification Accuracy: 0.8487704918032787\n",
      "Standard Deviation of Predictions: 1.2338950634002686\n",
      "F1 Score: 0.869195320808224\n",
      "Mean Absolute Error: 0.926927886716655\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=12, learning_rate=0.1, n_jobs=6)\n",
    "\n",
    "xgb_reg.fit(X[0:468480:32], Valence_Train[0:468480:32])\n",
    "\n",
    "\n",
    "predictions = xgb_reg.predict(M[0:78080:32])\n",
    "\n",
    "\n",
    "mse = mean_squared_error(Valence_Test[0:78080:32], predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "accuracy = accuracy_score((Valence_Test[0:78080:32] > 5), (predictions > 5))\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "std_dev = np.std(predictions)\n",
    "print(f\"Standard Deviation of Predictions: {std_dev}\")\n",
    "\n",
    "f1 = f1_score((Valence_Test[0:78080:32] > 5), (predictions > 5))\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "mae = mean_absolute_error(Valence_Test[0:78080:32], predictions)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.470086731851142\n",
      "Classification Accuracy: 0.840983606557377\n",
      "Standard Deviation of Predictions: 1.385977029800415\n",
      "F1 Score: 0.8687415426251691\n",
      "Mean Absolute Error: 0.9103075862526894\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=12, learning_rate=0.1, n_jobs=6)\n",
    "\n",
    "xgb_reg.fit(X[0:468480:32], Dominance_Train[0:468480:32])\n",
    "\n",
    "\n",
    "predictions = xgb_reg.predict(M[0:78080:32])\n",
    "\n",
    "\n",
    "mse = mean_squared_error(Dominance_Test[0:78080:32], predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "accuracy = accuracy_score((Dominance_Test[0:78080:32] > 5), (predictions > 5))\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "std_dev = np.std(predictions)\n",
    "print(f\"Standard Deviation of Predictions: {std_dev}\")\n",
    "\n",
    "f1 = f1_score((Dominance_Test[0:78080:32] > 5), (predictions > 5))\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "mae = mean_absolute_error(Dominance_Test[0:78080:32], predictions)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.9553648014891916\n",
      "Classification Accuracy: 0.8692622950819672\n",
      "Standard Deviation of Predictions: 1.3725663423538208\n",
      "F1 Score: 0.9031278469480717\n",
      "Mean Absolute Error: 1.0637404648436872\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=10, learning_rate=0.1, n_jobs=6)\n",
    "\n",
    "xgb_reg.fit(X[0:468480:32], Liking_Train[0:468480:32])\n",
    "\n",
    "\n",
    "predictions = xgb_reg.predict(M[0:78080:16])\n",
    "\n",
    "\n",
    "mse = mean_squared_error(Liking_Test[0:78080:16], predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "accuracy = accuracy_score((Liking_Test[0:78080:16] > 5), (predictions > 5))\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "std_dev = np.std(predictions)\n",
    "print(f\"Standard Deviation of Predictions: {std_dev}\")\n",
    "\n",
    "f1 = f1_score((Liking_Test[0:78080:16] > 5), (predictions > 5))\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "mae = mean_absolute_error(Liking_Test[0:78080:16], predictions)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, Classification Accuracy: 0.8379098360655738\n",
      "n_estimators: 300, Classification Accuracy: 0.8620901639344263\n",
      "n_estimators: 500, Classification Accuracy: 0.8637295081967213\n",
      "n_estimators: 600, Classification Accuracy: 0.8639344262295082\n",
      "n_estimators: 700, Classification Accuracy: 0.8639344262295082\n",
      "n_estimators: 800, Classification Accuracy: 0.8639344262295082\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# List of n_estimators to test\n",
    "n_estimators_list = [100,300,500, 600, 700, 800]\n",
    "\n",
    "# Loop through each n_estimators value\n",
    "for n in n_estimators_list:\n",
    "    # Initialize the model with the current n_estimators value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=n, max_depth=12, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Liking_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:16])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score((Liking_Test[0:78080:16] > 5), (predictions > 5))\n",
    "    print(f\"n_estimators: {n}, Classification Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (468480, 70)\n",
      "Shape of Valence_Train: (468480,)\n",
      "Shape of M: (78080, 70)\n",
      "Shape of Valence_Test: (78080,)\n",
      "n_estimators: 100, Classification Accuracy: 0.8172131147540984\n",
      "n_estimators: 300, Classification Accuracy: 0.8424180327868852\n",
      "n_estimators: 550, Classification Accuracy: 0.8440573770491804\n",
      "n_estimators: 650, Classification Accuracy: 0.8442622950819673\n",
      "n_estimators: 700, Classification Accuracy: 0.8442622950819673\n",
      "n_estimators: 800, Classification Accuracy: 0.8442622950819673\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Valence_Train: {Valence_Train.shape}\")\n",
    "print(f\"Shape of M: {M.shape}\")\n",
    "print(f\"Shape of Valence_Test: {Valence_Test.shape}\")\n",
    "\n",
    "# List of n_estimators to test\n",
    "n_estimators_list = [100, 300, 550, 650, 700,800]\n",
    "\n",
    "# Loop through each n_estimators value\n",
    "for n in n_estimators_list:\n",
    "    # Initialize the model with the current n_estimators value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=n, max_depth=12, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Valence_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:16])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score((Valence_Test[0:78080:16] > 5), (predictions > 5))\n",
    "    print(f\"n_estimators: {n}, Classification Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (468480, 70)\n",
      "Shape of Dominance_Train: (468480,)\n",
      "Shape of M: (78080, 70)\n",
      "Shape of Dominance_Test: (78080,)\n",
      "n_estimators: 100, Classification Accuracy: 0.8192622950819672\n",
      "n_estimators: 400, Classification Accuracy: 0.840983606557377\n",
      "n_estimators: 500, Classification Accuracy: 0.8413934426229508\n",
      "n_estimators: 600, Classification Accuracy: 0.8411885245901639\n",
      "n_estimators: 700, Classification Accuracy: 0.8413934426229508\n",
      "n_estimators: 800, Classification Accuracy: 0.8413934426229508\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Dominance_Train: {Dominance_Train.shape}\")\n",
    "print(f\"Shape of M: {M.shape}\")\n",
    "print(f\"Shape of Dominance_Test: {Dominance_Test.shape}\")\n",
    "\n",
    "# List of n_estimators to test\n",
    "n_estimators_list = [100,400,500, 600, 700, 800]\n",
    "\n",
    "# Loop through each n_estimators value\n",
    "for n in n_estimators_list:\n",
    "    # Initialize the model with the current n_estimators value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=n, max_depth=12, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Dominance_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:16])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score((Dominance_Test[0:78080:16] > 5), (predictions > 5))\n",
    "    print(f\"n_estimators: {n}, Classification Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (468480, 70)\n",
      "Shape of Arousal_Train: (468480,)\n",
      "Shape of M: (78080, 70)\n",
      "Shape of Arousal_Test: (78080,)\n",
      "n_estimators: 100, Classification Accuracy: 0.8135245901639344\n",
      "n_estimators: 400, Classification Accuracy: 0.8356557377049181\n",
      "n_estimators: 500, Classification Accuracy: 0.8364754098360656\n",
      "n_estimators: 600, Classification Accuracy: 0.8364754098360656\n",
      "n_estimators: 700, Classification Accuracy: 0.8364754098360656\n",
      "n_estimators: 800, Classification Accuracy: 0.8364754098360656\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Arousal_Train: {Arousal_Train.shape}\")\n",
    "print(f\"Shape of M: {M.shape}\")\n",
    "print(f\"Shape of Arousal_Test: {Arousal_Test.shape}\")\n",
    "\n",
    "# List of n_estimators to test\n",
    "n_estimators_list = [100,400,500, 600, 700, 800]\n",
    "\n",
    "# Loop through each n_estimators value\n",
    "for n in n_estimators_list:\n",
    "    # Initialize the model with the current n_estimators value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=n, max_depth=12, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Arousal_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:16])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score((Arousal_Test[0:78080:16] > 5), (predictions > 5))\n",
    "    print(f\"n_estimators: {n}, Classification Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (468480, 70)\n",
      "Shape of Liking_Train: (468480,)\n",
      "Shape of M: (78080, 70)\n",
      "Shape of Liking_Test: (78080,)\n",
      "Max Depth: 6\n",
      "  Classification Accuracy: 0.8342213114754098\n",
      "Max Depth: 8\n",
      "  Classification Accuracy: 0.8594262295081967\n",
      "Max Depth: 10\n",
      "  Classification Accuracy: 0.8692622950819672\n",
      "Max Depth: 12\n",
      "  Classification Accuracy: 0.8639344262295082\n",
      "Max Depth: 15\n",
      "  Classification Accuracy: 0.865983606557377\n",
      "Max Depth: 17\n",
      "  Classification Accuracy: 0.8633196721311476\n",
      "Max Depth: 19\n",
      "  Classification Accuracy: 0.8569672131147541\n",
      "Max Depth: 20\n",
      "  Classification Accuracy: 0.8532786885245902\n",
      "Max Depth: 21\n",
      "  Classification Accuracy: 0.855327868852459\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Liking_Train: {Liking_Train.shape}\")\n",
    "print(f\"Shape of M: {M.shape}\")\n",
    "print(f\"Shape of Liking_Test: {Liking_Test.shape}\")\n",
    "\n",
    "# List of max_depth values to test\n",
    "max_depth_list = [6, 8, 10, 12, 15,17,19,20,21]\n",
    "\n",
    "# Loop through each max_depth value\n",
    "for max_depth in max_depth_list:\n",
    "    # Initialize the model with the current max_depth value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=max_depth, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Liking_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:16])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score((Liking_Test[0:78080:16] > 5), (predictions > 5))\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "\n",
    "    print(f\"  Classification Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (468480, 70)\n",
      "Shape of Valence_Train: (468480,)\n",
      "Shape of M: (78080, 70)\n",
      "Shape of Valence_Test: (78080,)\n",
      "Max Depth: 10\n",
      "  Classification Accuracy: 0.840983606557377\n",
      "Max Depth: 12\n",
      "  Classification Accuracy: 0.8442622950819673\n",
      "Max Depth: 15\n",
      "  Classification Accuracy: 0.8444672131147541\n",
      "Max Depth: 16\n",
      "  Classification Accuracy: 0.8440573770491804\n",
      "Max Depth: 17\n",
      "  Classification Accuracy: 0.8391393442622951\n",
      "Max Depth: 18\n",
      "  Classification Accuracy: 0.8315573770491803\n",
      "Max Depth: 19\n",
      "  Classification Accuracy: 0.8422131147540983\n",
      "Max Depth: 20\n",
      "  Classification Accuracy: 0.832172131147541\n",
      "Max Depth: 21\n",
      "  Classification Accuracy: 0.833811475409836\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Example initialization of Valence_Train and Valence_Test\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Valence_Train: {Valence_Train.shape}\")\n",
    "print(f\"Shape of M: {M.shape}\")\n",
    "print(f\"Shape of Valence_Test: {Valence_Test.shape}\")\n",
    "\n",
    "# List of max_depth values to test\n",
    "max_depth_list = [ 10, 12, 15,16,17,18,19,20,21]\n",
    "\n",
    "# Loop through each max_depth value\n",
    "for max_depth in max_depth_list:\n",
    "    # Initialize the model with the current max_depth value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=max_depth, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Valence_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:16])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score((Valence_Test[0:78080:16] > 5), (predictions > 5))\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "\n",
    "    print(f\"  Classification Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 6\n",
      "  Classification Accuracy: 0.8065573770491803\n",
      "\n",
      "Max Depth: 8\n",
      "  Classification Accuracy: 0.8274590163934427\n",
      "\n",
      "Max Depth: 10\n",
      "  Classification Accuracy: 0.8336065573770491\n",
      "\n",
      "Max Depth: 12\n",
      "  Classification Accuracy: 0.8360655737704918\n",
      "\n",
      "Max Depth: 15\n",
      "  Classification Accuracy: 0.8377049180327869\n",
      "\n",
      "Max Depth: 17\n",
      "  Classification Accuracy: 0.8389344262295082\n",
      "\n",
      "Max Depth: 19\n",
      "  Classification Accuracy: 0.8348360655737705\n",
      "\n",
      "Max Depth: 20\n",
      "  Classification Accuracy: 0.8315573770491803\n",
      "\n",
      "Max Depth: 21\n",
      "  Classification Accuracy: 0.8282786885245902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# List of max_depth values to test\n",
    "max_depth_list = [6, 8, 10, 12, 15,17,19,20,21]\n",
    "\n",
    "# Loop through each max_depth value\n",
    "for max_depth in max_depth_list:\n",
    "    # Initialize the model with the current max_depth value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=max_depth, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Arousal_Train[0:468480:32])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:32])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score((Arousal_Test[0:78080:32] > 5), (predictions > 5))\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "    print(f\"  Classification Accuracy: {accuracy}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 6\n",
      "  Classification Accuracy: 0.8106557377049181\n",
      "\n",
      "Max Depth: 8\n",
      "  Classification Accuracy: 0.8352459016393443\n",
      "\n",
      "Max Depth: 10\n",
      "  Classification Accuracy: 0.839344262295082\n",
      "\n",
      "Max Depth: 12\n",
      "  Classification Accuracy: 0.840983606557377\n",
      "\n",
      "Max Depth: 15\n",
      "  Classification Accuracy: 0.8401639344262295\n",
      "\n",
      "Max Depth: 17\n",
      "  Classification Accuracy: 0.8381147540983607\n",
      "\n",
      "Max Depth: 19\n",
      "  Classification Accuracy: 0.8426229508196721\n",
      "\n",
      "Max Depth: 20\n",
      "  Classification Accuracy: 0.839344262295082\n",
      "\n",
      "Max Depth: 21\n",
      "  Classification Accuracy: 0.8233606557377049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have data arrays similar to Arousal_Train, Arousal_Test, and M for dominance\n",
    "\n",
    "# List of max_depth values to test\n",
    "max_depth_list = [6, 8, 10, 12, 15, 17, 19, 20, 21]\n",
    "\n",
    "# Loop through each max_depth value\n",
    "for max_depth in max_depth_list:\n",
    "    # Initialize the model with the current max_depth value\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=800, max_depth=max_depth, learning_rate=0.1, n_jobs=6)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_reg.fit(X[0:468480:32], Dominance_Train[0:468480:32])  # Assuming Dominance_Train is your training data for dominance\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = xgb_reg.predict(M[0:78080:32])  # Assuming M is your test data for dominance\n",
    "    \n",
    "    # Since this is regression, you might want to threshold predictions for classification purposes\n",
    "    # Example: Assuming dominance values > 5 are considered high dominance\n",
    "    dominance_predictions = (predictions > 5)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score((Dominance_Test[0:78080:32] > 5), dominance_predictions)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "    print(f\"  Classification Accuracy: {accuracy}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
