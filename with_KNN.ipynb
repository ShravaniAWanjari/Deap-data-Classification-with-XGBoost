{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EEG processing parameters\n",
    "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] # 14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] # 5 bands\n",
    "window_size = 256 # Averaging band power of 2 sec\n",
    "step_size = 16 # Each 0.125 sec update once\n",
    "sample_rate = 128 # Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20']#,'21','22','23','24','25','26','27','28','29','30','31','32'] # List of subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "data_validation = []\n",
    "label_validation = []\n",
    "\n",
    "for subjects in subjectList:\n",
    "    file_path = os.path.join('out', f's{subjects}.npy')\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        sub = np.load(file, allow_pickle=True)\n",
    "        for i in range(sub.shape[0]):\n",
    "            if i % 8 == 0:\n",
    "                data_testing.append(sub[i][0])\n",
    "                label_testing.append(sub[i][1])\n",
    "            elif i % 8 == 1:\n",
    "                data_validation.append(sub[i][0])\n",
    "                label_validation.append(sub[i][1])\n",
    "            else:\n",
    "                data_training.append(sub[i][0])\n",
    "                label_training.append(sub[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset: (292800, 70) (292800, 4)\n",
      "testing dataset: (48800, 70) (48800, 4)\n",
      "validation dataset: (48800, 70) (48800, 4)\n"
     ]
    }
   ],
   "source": [
    "np.save(os.path.join('out', 'data_training'), np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save(os.path.join('out', 'label_training'), np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "np.save(os.path.join('out', 'data_testing'), np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save(os.path.join('out', 'label_testing'), np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
    "\n",
    "np.save(os.path.join('out', 'data_validation'), np.array(data_validation), allow_pickle=True, fix_imports=True)\n",
    "np.save(os.path.join('out', 'label_validation'), np.array(label_validation), allow_pickle=True, fix_imports=True)\n",
    "print(\"validation dataset:\", np.array(data_validation).shape, np.array(label_validation).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "with open('out/data_training.npy', 'rb') as fileTrain:\n",
    "    X_train = np.load(fileTrain)\n",
    "\n",
    "with open('out/label_training.npy', 'rb') as fileTrainL:\n",
    "    Y_train = np.load(fileTrainL)\n",
    "\n",
    "# Loading validation data\n",
    "with open('out/data_validation.npy', 'rb') as fileValid:\n",
    "    X_val = np.load(fileValid)\n",
    "\n",
    "with open('out/label_validation.npy', 'rb') as fileValidL:\n",
    "    Y_val = np.load(fileValidL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizing data...\")\n",
    "X_train_normalized = normalize(X_train)\n",
    "X_val_normalized = normalize(X_val)\n",
    "\n",
    "# Extracting individual labels\n",
    "Valence_train = np.ravel(Y_train[:, [1]])\n",
    "Arousal_train = np.ravel(Y_train[:, [0]])\n",
    "Dominance_train = np.ravel(Y_train[:, [2]])\n",
    "Liking_train = np.ravel(Y_train[:, [3]])\n",
    "\n",
    "Valence_val = np.ravel(Y_val[:, [1]])\n",
    "Arousal_val = np.ravel(Y_val[:, [0]])\n",
    "Dominance_val = np.ravel(Y_val[:, [2]])\n",
    "Liking_val = np.ravel(Y_val[:, [3]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence - Validation Mean Squared Error (MSE): 1.3691196316075611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "knn_valence = KNeighborsRegressor(n_neighbors=7, weights=\"uniform\", p=1, algorithm='brute')\n",
    "knn_valence.fit(X_train_normalized, Valence_train)\n",
    "valence_predictions = knn_valence.predict(X_val_normalized)\n",
    "valence_mse = mean_squared_error(Valence_val, valence_predictions)\n",
    "print(\"Valence - Validation Mean Squared Error (MSE):\", valence_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arousal - Validation Mean Squared Error (MSE): 1.6473423354382737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNN Model for Arousal\n",
    "knn_arousal = KNeighborsRegressor(n_neighbors=7, weights='uniform',p=1,algorithm='brute')\n",
    "knn_arousal.fit(X_train_normalized, Arousal_train)\n",
    "arousal_predictions = knn_arousal.predict(X_val_normalized)\n",
    "arousal_mse = mean_squared_error(Arousal_val, arousal_predictions)\n",
    "print(\"Arousal - Validation Mean Squared Error (MSE):\", arousal_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominance - Validation Mean Squared Error (MSE): 1.457959044998327\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNN Model for Dominance\n",
    "knn_dominance = KNeighborsRegressor(n_neighbors=7, weights='uniform',p=1, algorithm='brute')\n",
    "knn_dominance.fit(X_train_normalized, Dominance_train)\n",
    "dominance_predictions = knn_dominance.predict(X_val_normalized)\n",
    "dominance_mse = mean_squared_error(Dominance_val, dominance_predictions)\n",
    "print(\"Dominance - Validation Mean Squared Error (MSE):\", dominance_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liking - Validation Mean Squared Error (MSE): 1.4514042467385733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNN Model for Liking\n",
    "knn_liking = KNeighborsRegressor(n_neighbors=7, weights='distance',p=1, algorithm='brute')\n",
    "knn_liking.fit(X_train_normalized, Liking_train)\n",
    "liking_predictions = knn_liking.predict(X_val_normalized)\n",
    "liking_mse = mean_squared_error(Liking_val, liking_predictions)\n",
    "print(\"Liking - Validation Mean Squared Error (MSE):\", liking_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for Liking...\n",
      "Liking - Validation Mean Squared Error (MSE): 1.4514042467385733\n",
      "Liking - Validation Mean Absolute Error (MAE): 0.7670405995878238\n",
      "Liking - Validation Accuracy: 0.8337909836065573\n",
      "Liking - Validation F1-score: 0.832933434012569\n",
      "Liking - Validation Std_dev: 0.9290064398713956\n",
      "\n",
      "Calculating metrics for Dominance...\n",
      "Dominance - Validation Mean Squared Error (MSE): 1.457959044998327\n",
      "Dominance - Validation Mean Absolute Error (MAE): 0.8090478337236535\n",
      "Dominance - Validation Accuracy: 0.825266393442623\n",
      "Dominance - Validation F1-score: 0.8254074903906702\n",
      "Dominance - Validation Std_dev: 0.8963261949454512\n",
      "\n",
      "Calculating metrics for Arousal...\n",
      "Arousal - Validation Mean Squared Error (MSE): 1.6473423354382737\n",
      "Arousal - Validation Mean Absolute Error (MAE): 0.8801151932084309\n",
      "Arousal - Validation Accuracy: 0.8212090163934426\n",
      "Arousal - Validation F1-score: 0.8209727754751492\n",
      "Arousal - Validation Std_dev: 0.9342053211804994\n",
      "\n",
      "Calculating metrics for Valence...\n",
      "Valence - Validation Mean Squared Error (MSE): 1.3691196316075611\n",
      "Valence - Validation Mean Absolute Error (MAE): 0.7927560597189695\n",
      "Valence - Validation Accuracy: 0.8262704918032787\n",
      "Valence - Validation F1-score: 0.8262431639762532\n",
      "Valence - Validation Std_dev: 0.8606145835311034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Function to binarize labels based on the median value as a threshold\n",
    "def binarize_labels(labels, threshold=None):\n",
    "    if threshold is None:\n",
    "        threshold = np.median(labels)\n",
    "    return np.where(labels >= threshold, 1, 0)\n",
    "\n",
    "# Function to calculate and print metrics\n",
    "def calculate_and_print_metrics(true_labels, predictions, feature_name):\n",
    "    mse = mean_squared_error(true_labels, predictions)\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "    threshold = np.median(true_labels)  # Using median of true labels as threshold\n",
    "    true_classes = binarize_labels(true_labels, threshold)\n",
    "    predicted_classes = binarize_labels(predictions, threshold)\n",
    "    \n",
    "    accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    f1score = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "    std_dev = np.std(np.abs(true_labels - predictions))\n",
    "    \n",
    "    print(f\"{feature_name} - Validation Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"{feature_name} - Validation Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"{feature_name} - Validation Accuracy: {accuracy}\")\n",
    "    print(f\"{feature_name} - Validation F1-score: {f1score}\")\n",
    "    print(f\"{feature_name} - Validation Std_dev: {std_dev}\")\n",
    "    print()\n",
    "\n",
    "# Calculate and print metrics for all features\n",
    "print(\"Calculating metrics for Liking...\")\n",
    "calculate_and_print_metrics(Liking_val, liking_predictions, \"Liking\")\n",
    "\n",
    "print(\"Calculating metrics for Dominance...\")\n",
    "calculate_and_print_metrics(Dominance_val, dominance_predictions, \"Dominance\")\n",
    "\n",
    "print(\"Calculating metrics for Arousal...\")\n",
    "calculate_and_print_metrics(Arousal_val, arousal_predictions, \"Arousal\")\n",
    "\n",
    "print(\"Calculating metrics for Valence...\")\n",
    "calculate_and_print_metrics(Valence_val, valence_predictions, \"Valence\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
